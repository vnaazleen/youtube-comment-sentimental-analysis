{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d0c392",
   "metadata": {},
   "source": [
    "# YouTube Comments Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdd7a6",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "22a613cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd; import os\n",
    "import csv; import numpy as np\n",
    "import re; import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52c03c",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "### Reading Testing YouTube Video Comments\n",
    "\n",
    "data.csv files has comments of youtube video -- https://www.youtube.com/watch?v=kfVsfOSbJY0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b821e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get Fact Right: In India School have asked stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All these laws seem to be created to suppress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This will change something because its the fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How is it same in case of India??? Did the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agreed.  Keep the fight alive ladies. Remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  Get Fact Right: In India School have asked stu...\n",
       "1  All these laws seem to be created to suppress ...\n",
       "2  This will change something because its the fir...\n",
       "3  How is it same in case of India??? Did the sta...\n",
       "4  Agreed.  Keep the fight alive ladies. Remember..."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_comments = pd.read_csv('Comments.csv', delimiter=\",\", encoding='utf-8', engine='python')\n",
    "prediction_comments = prediction_comments.iloc[:,:1]\n",
    "prediction_comments.columns=['comment']\n",
    "prediction_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a596346",
   "metadata": {},
   "source": [
    "### Reading Pre-Labeled YouTube Video Comments\n",
    "\n",
    "here we are taking pre-labeled comments of 5 popular youtube videos for training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fd5c0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "okgo = pd.read_csv('OKGO.csv', delimiter=\";\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "trump = pd.read_csv('trump.csv', delimiter=\",\", skiprows=2, encoding='utf-8', error_bad_lines=False, engine='python')\n",
    "swift = pd.read_csv('TaylorSwift.csv', delimiter=\",\", skiprows=2, nrows=180, encoding='utf-8', engine='python')\n",
    "royal = pd.read_csv('RoyalWedding.csv', delimiter=\",\", skiprows=2, nrows=61, encoding='utf-8', engine='python')\n",
    "paul = pd.read_csv('LoganPaul.csv', delimiter=\",\", skiprows=2, nrows=200, encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928ae40",
   "metadata": {},
   "source": [
    "### Reading Pre-Labeled Tweets & Blog Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "264c6049",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = pd.read_csv('Kagel.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') # read in the data\n",
    "tweets = pd.read_csv('twitter.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') # read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138f3ca",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7eea8c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Now all @Apple has to do is get swype on the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@Apple will be adding more carrier support to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>@RIM you made it too easy for me to switch to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>I just realized that the reason I got into twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                          TweetText\n",
       "0  positive  Now all @Apple has to do is get swype on the i...\n",
       "1  positive  @Apple will be adding more carrier support to ...\n",
       "2  positive  Hilarious @youtube video - guy does a duet wit...\n",
       "3  positive  @RIM you made it too easy for me to switch to ...\n",
       "4  positive  I just realized that the reason I got into twi..."
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean dataframes\n",
    "tweets = tweets.drop(['Topic', 'TweetId', \"TweetDate\"], axis = 1).dropna()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1a01ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_cols(DF):\n",
    "    DF = DF.iloc[:,:2]\n",
    "    DF.columns = [\"label\", \"comment\"]\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d8820145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand's papers from.\\rBut -No o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ÒYour paper cut balance is: \\r-£25279102771Ó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE.........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Blowing my mind yet again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Should have gone with Dunder Mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0   -1.0  Everyone knows brand's papers from.\\rBut -No o...\n",
       "1    0.0       ÒYour paper cut balance is: \\r-£25279102771Ó\n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE.........\n",
       "3    1.0                          Blowing my mind yet again\n",
       "4    0.0               Should have gone with Dunder Mifflin"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okgo = fix_cols(okgo)\n",
    "trump = fix_cols(trump)\n",
    "swift = fix_cols(swift)\n",
    "royal = fix_cols(royal)\n",
    "paul = fix_cols(paul)\n",
    "tweets = fix_cols(tweets)\n",
    "\n",
    "\n",
    "okgo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "533447a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.label = tweets.label.replace({'positive': '1.0', 'negative':'-1.0', 'neutral': '0.0', 'irrelevant': '0.0'}, regex=True)\n",
    "tweets['label'] = pd.to_numeric(tweets['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0e128ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Now all @Apple has to do is get swype on the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@Apple will be adding more carrier support to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>@RIM you made it too easy for me to switch to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I just realized that the reason I got into twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0    1.0  Now all @Apple has to do is get swype on the i...\n",
       "1    1.0  @Apple will be adding more carrier support to ...\n",
       "2    1.0  Hilarious @youtube video - guy does a duet wit...\n",
       "3    1.0  @RIM you made it too easy for me to switch to ...\n",
       "4    1.0  I just realized that the reason I got into twi..."
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = fix_cols(tweets)\n",
    "blogs = fix_cols(blogs)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b608f",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e909aaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand's papers from.\\rBut -No o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ÒYour paper cut balance is: \\r-£25279102771Ó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE.........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Blowing my mind yet again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Should have gone with Dunder Mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0   -1.0  Everyone knows brand's papers from.\\rBut -No o...\n",
       "1    0.0       ÒYour paper cut balance is: \\r-£25279102771Ó\n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE.........\n",
       "3    1.0                          Blowing my mind yet again\n",
       "4    0.0               Should have gone with Dunder Mifflin"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_comments = pd.concat([okgo, trump, swift, royal, paul], ignore_index=True)\n",
    "yt_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "92cf2f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>i liked the Da Vinci Code a lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>i liked the Da Vinci Code a lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>that's not even an exaggeration ) and at midn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I loved the Da Vinci Code but now I want some...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0    1.0                    i liked the Da Vinci Code a lot\n",
       "1    1.0                    i liked the Da Vinci Code a lot\n",
       "2    1.0   I liked the Da Vinci Code but it ultimatly di...\n",
       "3    1.0   that's not even an exaggeration ) and at midn...\n",
       "4    1.0   I loved the Da Vinci Code but now I want some..."
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_yt_comments = pd.concat([blogs, tweets], ignore_index=True)\n",
    "non_yt_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cee5a6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand's papers from.\\rBut -No o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>ÒYour paper cut balance is: \\r-£25279102771Ó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE.........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Blowing my mind yet again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Should have gone with Dunder Mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0   -1.0  Everyone knows brand's papers from.\\rBut -No o...\n",
       "1    0.0       ÒYour paper cut balance is: \\r-£25279102771Ó\n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE.........\n",
       "3    1.0                          Blowing my mind yet again\n",
       "4    0.0               Should have gone with Dunder Mifflin"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.concat([yt_comments, non_yt_comments], ignore_index=True)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d25d3",
   "metadata": {},
   "source": [
    "### Remove Non-Alphabetic Characters (including numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "04216102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(DF):\n",
    "    DF[\"comment\"]= DF[\"comment\"].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dcc4e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_string(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fd9cfdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanerFn(b):\n",
    "    # keeps only words with alphabetic characters in comments\n",
    "    for row in range(len(b)):\n",
    "        line = b.loc[row, \"comment\"]\n",
    "        b.loc[row,\"comment\"] = re.sub(\"[^a-zA-Z]\", \" \", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "50219d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand s papers from  But  No on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Your paper cut balance is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Blowing my mind yet again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Should have gone with Dunder Mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0   -1.0  Everyone knows brand s papers from  But  No on...\n",
       "1    0.0         Your paper cut balance is                 \n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...\n",
       "3    1.0                          Blowing my mind yet again\n",
       "4    0.0               Should have gone with Dunder Mifflin"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanerFn(comments)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94f1bb",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "84bdefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f01daee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187d77e",
   "metadata": {},
   "source": [
    "#### Tokenization, Remove Stop Words, Lemmatization & Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b6de5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlpFunction(DF):\n",
    "    DF['com_token'] = DF['comment'].str.lower().str.split()\n",
    "    DF['com_remv'] = DF['com_token'].apply(lambda x: [y for y in x if y not in sw])\n",
    "    DF[\"com_lemma\"] = DF['com_remv'].apply(lambda x : [lemmatizer.lemmatize(y) for y in x]) # lemmatization\n",
    "    DF['com_stem'] = DF['com_lemma'].apply(lambda x : [ps.stem(y) for y in x]) # stemming\n",
    "    DF[\"com_tok_str\"] = DF[\"com_stem\"].apply(', '.join)\n",
    "    DF[\"com_full\"] = DF[\"com_remv\"].apply(' '.join)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7f2ac4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_tok_str</th>\n",
       "      <th>com_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand s papers from  But  No on...</td>\n",
       "      <td>[everyone, knows, brand, s, papers, from, but,...</td>\n",
       "      <td>[everyone, knows, brand, papers, one, knows, w...</td>\n",
       "      <td>[everyone, know, brand, paper, one, know, welf...</td>\n",
       "      <td>[everyon, know, brand, paper, one, know, welfa...</td>\n",
       "      <td>everyon, know, brand, paper, one, know, welfar...</td>\n",
       "      <td>everyone knows brand papers one knows welfare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Your paper cut balance is</td>\n",
       "      <td>[your, paper, cut, balance, is]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balance]</td>\n",
       "      <td>[paper, cut, balanc]</td>\n",
       "      <td>paper, cut, balanc</td>\n",
       "      <td>paper cut balance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...</td>\n",
       "      <td>[oh, shit, when, i, saw, this, on, my, front, ...</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>[oh, shit, saw, front, page, love, song]</td>\n",
       "      <td>oh, shit, saw, front, page, love, song</td>\n",
       "      <td>oh shit saw front page love song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Blowing my mind yet again</td>\n",
       "      <td>[blowing, my, mind, yet, again]</td>\n",
       "      <td>[blowing, mind, yet]</td>\n",
       "      <td>[blowing, mind, yet]</td>\n",
       "      <td>[blow, mind, yet]</td>\n",
       "      <td>blow, mind, yet</td>\n",
       "      <td>blowing mind yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Should have gone with Dunder Mifflin</td>\n",
       "      <td>[should, have, gone, with, dunder, mifflin]</td>\n",
       "      <td>[gone, dunder, mifflin]</td>\n",
       "      <td>[gone, dunder, mifflin]</td>\n",
       "      <td>[gone, dunder, mifflin]</td>\n",
       "      <td>gone, dunder, mifflin</td>\n",
       "      <td>gone dunder mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment  \\\n",
       "0   -1.0  Everyone knows brand s papers from  But  No on...   \n",
       "1    0.0         Your paper cut balance is                    \n",
       "2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...   \n",
       "3    1.0                          Blowing my mind yet again   \n",
       "4    0.0               Should have gone with Dunder Mifflin   \n",
       "\n",
       "                                           com_token  \\\n",
       "0  [everyone, knows, brand, s, papers, from, but,...   \n",
       "1                    [your, paper, cut, balance, is]   \n",
       "2  [oh, shit, when, i, saw, this, on, my, front, ...   \n",
       "3                    [blowing, my, mind, yet, again]   \n",
       "4        [should, have, gone, with, dunder, mifflin]   \n",
       "\n",
       "                                            com_remv  \\\n",
       "0  [everyone, knows, brand, papers, one, knows, w...   \n",
       "1                              [paper, cut, balance]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "3                               [blowing, mind, yet]   \n",
       "4                            [gone, dunder, mifflin]   \n",
       "\n",
       "                                           com_lemma  \\\n",
       "0  [everyone, know, brand, paper, one, know, welf...   \n",
       "1                              [paper, cut, balance]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "3                               [blowing, mind, yet]   \n",
       "4                            [gone, dunder, mifflin]   \n",
       "\n",
       "                                            com_stem  \\\n",
       "0  [everyon, know, brand, paper, one, know, welfa...   \n",
       "1                               [paper, cut, balanc]   \n",
       "2           [oh, shit, saw, front, page, love, song]   \n",
       "3                                  [blow, mind, yet]   \n",
       "4                            [gone, dunder, mifflin]   \n",
       "\n",
       "                                         com_tok_str  \\\n",
       "0  everyon, know, brand, paper, one, know, welfar...   \n",
       "1                                 paper, cut, balanc   \n",
       "2             oh, shit, saw, front, page, love, song   \n",
       "3                                    blow, mind, yet   \n",
       "4                              gone, dunder, mifflin   \n",
       "\n",
       "                                            com_full  \n",
       "0  everyone knows brand papers one knows welfare ...  \n",
       "1                                  paper cut balance  \n",
       "2                   oh shit saw front page love song  \n",
       "3                                   blowing mind yet  \n",
       "4                                gone dunder mifflin  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = nlpFunction(comments)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "249f017d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>com_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>everyone knows brand papers one knows welfare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>paper cut balance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>oh shit saw front page love song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>blowing mind yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>gone dunder mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           com_full\n",
       "0   -1.0  everyone knows brand papers one knows welfare ...\n",
       "1    0.0                                  paper cut balance\n",
       "2    1.0                   oh shit saw front page love song\n",
       "3    1.0                                   blowing mind yet\n",
       "4    0.0                                gone dunder mifflin"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_cols_after_nlp(comments):\n",
    "    comments = comments.drop(columns = ['comment', 'com_token', 'com_remv', 'com_lemma', 'com_stem', 'com_tok_str'], axis = 1)\n",
    "    return comments\n",
    "comments = drop_cols_after_nlp(comments)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2a2ddfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>everyone knows brand papers one knows welfare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>paper cut balance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>oh shit saw front page love song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>blowing mind yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>gone dunder mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0   -1.0  everyone knows brand papers one knows welfare ...\n",
       "1    0.0                                  paper cut balance\n",
       "2    1.0                   oh shit saw front page love song\n",
       "3    1.0                                   blowing mind yet\n",
       "4    0.0                                gone dunder mifflin"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.rename(columns = {'com_full': 'comment'}, inplace=True)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "90fe2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_vals(comments): \n",
    "    comments['comment'] = comments['comment'].str.strip()\n",
    "    comments = comments[comments.comment != 'nan'] # remove nan values from data\n",
    "    comments = comments[comments.comment != '']\n",
    "    \n",
    "remove_missing_vals(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "923090e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>everyone knows brand papers one knows welfare ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>paper cut balance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>oh shit saw front page love song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>blowing mind yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>gone dunder mifflin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment\n",
       "0   -1.0  everyone knows brand papers one knows welfare ...\n",
       "1    0.0                                  paper cut balance\n",
       "2    1.0                   oh shit saw front page love song\n",
       "3    1.0                                   blowing mind yet\n",
       "4    0.0                                gone dunder mifflin"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cbcc2ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2355"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['label'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b3bc396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = comments[comments['label'].notna()]\n",
    "comments['label'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "318c8c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14830"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f0531a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comments['comment']\n",
    "y = comments.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "258b7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=53, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691e17d",
   "metadata": {},
   "source": [
    "### Vectorize the tweets\n",
    "<p>We have the training and testing data all set up, but we need to create vectorized representations of the tweets in order to apply machine learning.</p>\n",
    "<p>To do so, we will utilize the <code>CountVectorizer</code> and <code>TfidfVectorizer</code> classes which we will first need to fit to the data.</p>\n",
    "<p>Once this is complete, we can start modeling with the new vectorized tweets!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "91258b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize count vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english', \n",
    "                                   min_df=0.05, max_df=0.9)\n",
    "\n",
    "# Create count train and test variables\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                   min_df=0.05, max_df=0.9)\n",
    "\n",
    "# Create tfidf train and test variables\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48a0756",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ec3c1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "import random; random.seed(5)\n",
    "\n",
    "# Import all we need from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b0c7d",
   "metadata": {},
   "source": [
    "### Multinomial Naive-Bayes Model\n",
    "Training a multinomial naive Bayes model\n",
    "<p>Now that we have the data in vectorized form, we can train the first model. Investigate using the Multinomial Naive Bayes model with both the <code>CountVectorizer</code> and <code>TfidfVectorizer</code> data. Which do will perform better? How come?</p>\n",
    "<p>To assess the accuracies, we will print the test sets accuracy scores for both models.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0dd136e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Tfidf Score:  0.7909924487594391\n",
      "NaiveBayes Count Score:  0.7831715210355987\n"
     ]
    }
   ],
   "source": [
    "# Create a MulitnomialNB model\n",
    "tfidf_nb = MultinomialNB()\n",
    "tfidf_nb.fit(tfidf_train,y_train)\n",
    "# Run predict on your TF-IDF test data to get your predictions\n",
    "tfidf_nb_pred = tfidf_nb.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy of your predictions\n",
    "tfidf_nb_score = metrics.accuracy_score(y_test,tfidf_nb_pred)\n",
    "\n",
    "# Create a MulitnomialNB model\n",
    "count_nb = MultinomialNB()\n",
    "count_nb.fit(count_train,y_train)\n",
    "\n",
    "# Run predict on your count test data to get your predictions\n",
    "count_nb_pred = count_nb.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy of your predictions\n",
    "count_nb_score = metrics.accuracy_score(count_nb_pred,y_test)\n",
    "\n",
    "print('NaiveBayes Tfidf Score: ', tfidf_nb_score)\n",
    "print('NaiveBayes Count Score: ', count_nb_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91191ff",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1001a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Score:   0.798\n"
     ]
    }
   ],
   "source": [
    "# Create a SVM model\n",
    "tfidf_svc = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo')\n",
    "\n",
    "tfidf_svc.fit(tfidf_train,y_train)\n",
    "# Run predict on your tfidf test data to get your predictions\n",
    "tfidf_svc_pred = tfidf_svc.predict(tfidf_test)\n",
    "\n",
    "# Calculate your accuracy using the metrics module\n",
    "tfidf_svc_score = metrics.accuracy_score(y_test,tfidf_svc_pred)\n",
    "\n",
    "print(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b8d8cf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'waste ink paper'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3b581a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tfidf_svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c022176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get Fact Right: In India School have asked stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All these laws seem to be created to suppress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This will change something because its the fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How is it same in case of India??? Did the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agreed.  Keep the fight alive ladies. Remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  Get Fact Right: In India School have asked stu...\n",
       "1  All these laws seem to be created to suppress ...\n",
       "2  This will change something because its the fir...\n",
       "3  How is it same in case of India??? Did the sta...\n",
       "4  Agreed.  Keep the fight alive ladies. Remember..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets use SVC to predict on our youtube video comments\n",
    "prediction_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b7398f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction_comments['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1cf54e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>com_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get fact right india school asked students wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>laws seem created suppress women shame women t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>change something first time persians kurds rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case india state karnataka govt passed law uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agreed keep fight alive ladies remember win st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            com_full\n",
       "0  get fact right india school asked students wea...\n",
       "1  laws seem created suppress women shame women t...\n",
       "2  change something first time persians kurds rai...\n",
       "3  case india state karnataka govt passed law uni...\n",
       "4  agreed keep fight alive ladies remember win st..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_string(prediction_comments)\n",
    "cleanerFn(prediction_comments)\n",
    "prediction_comments = nlpFunction(prediction_comments)\n",
    "prediction_comments = drop_cols_after_nlp(prediction_comments)\n",
    "prediction_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a02ad9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_comments.rename(columns = {'com_full': 'comment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ce6f62e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get fact right india school asked students wea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>laws seem created suppress women shame women t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>change something first time persians kurds rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case india state karnataka govt passed law uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agreed keep fight alive ladies remember win st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  get fact right india school asked students wea...\n",
       "1  laws seem created suppress women shame women t...\n",
       "2  change something first time persians kurds rai...\n",
       "3  case india state karnataka govt passed law uni...\n",
       "4  agreed keep fight alive ladies remember win st..."
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_missing_vals(prediction_comments)\n",
    "prediction_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94dddc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pred = tfidf_vectorizer.transform(prediction_comments['comment'])\n",
    "tfidf_svc_pred = tfidf_svc.predict(tfidf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6d1d0f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral = (tfidf_svc_pred == 0.0).sum()\n",
    "positive = (tfidf_svc_pred == 1.0).sum()\n",
    "negative = (tfidf_svc_pred < 0).sum()\n",
    "\n",
    "len(tfidf_svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7a5251c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 5 0\n"
     ]
    }
   ],
   "source": [
    "print(neutral, positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9f6298d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good video\n"
     ]
    }
   ],
   "source": [
    "print(\"Good video\" if positive > negative else \"Bad video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f937d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
